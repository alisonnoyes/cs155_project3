{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Softmax\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 37\n",
      "nb sequences: 31211\n"
     ]
    }
   ],
   "source": [
    "# # Load semi-redundant sequences of 40 characters from the sonnet corpus    \n",
    "filename = 'data/shakespeare.txt'\n",
    "\n",
    "file_str = \"\"\n",
    "with open(filename, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for line in content:\n",
    "        if len(line) > 23:\n",
    "            line = line.strip('  ')\n",
    "            file_str += line.replace('\\n', ' ').lower()\n",
    "text = file_str\n",
    "            \n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation=\"softmax\"))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "31211/31211 [==============================] - 42s 1ms/step - loss: 0.3590\n",
      "Epoch 2/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.3200\n",
      "Epoch 3/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.3026\n",
      "Epoch 4/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2927\n",
      "Epoch 5/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2844\n",
      "Epoch 6/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2816\n",
      "Epoch 7/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2725\n",
      "Epoch 8/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2684\n",
      "Epoch 9/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.2621\n",
      "Epoch 10/100\n",
      "31211/31211 [==============================] - 44s 1ms/step - loss: 0.2614\n",
      "Epoch 11/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.2572\n",
      "Epoch 12/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2553\n",
      "Epoch 13/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2539\n",
      "Epoch 14/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.2513\n",
      "Epoch 15/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2514\n",
      "Epoch 16/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.2420\n",
      "Epoch 17/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2341\n",
      "Epoch 18/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2517\n",
      "Epoch 19/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2341\n",
      "Epoch 20/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2312\n",
      "Epoch 21/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2269\n",
      "Epoch 22/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2224\n",
      "Epoch 23/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2260\n",
      "Epoch 24/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2257\n",
      "Epoch 25/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2211\n",
      "Epoch 26/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2165\n",
      "Epoch 27/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2179\n",
      "Epoch 28/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.2147\n",
      "Epoch 29/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2150\n",
      "Epoch 30/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2085\n",
      "Epoch 31/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2112\n",
      "Epoch 32/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2165\n",
      "Epoch 33/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2085\n",
      "Epoch 34/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2135\n",
      "Epoch 35/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2041\n",
      "Epoch 36/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2078\n",
      "Epoch 37/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.2013\n",
      "Epoch 38/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2009\n",
      "Epoch 39/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2052\n",
      "Epoch 40/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1994\n",
      "Epoch 41/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.2017\n",
      "Epoch 42/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1984\n",
      "Epoch 43/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1972\n",
      "Epoch 44/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1962\n",
      "Epoch 45/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1963\n",
      "Epoch 46/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1941\n",
      "Epoch 47/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1924\n",
      "Epoch 48/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1886\n",
      "Epoch 49/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1912\n",
      "Epoch 50/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1891\n",
      "Epoch 51/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1874\n",
      "Epoch 52/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1881\n",
      "Epoch 53/100\n",
      "31211/31211 [==============================] - 37s 1ms/step - loss: 0.1883\n",
      "Epoch 54/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1889\n",
      "Epoch 55/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1844\n",
      "Epoch 56/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1859\n",
      "Epoch 57/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1805\n",
      "Epoch 58/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1825\n",
      "Epoch 59/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1821\n",
      "Epoch 60/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1784\n",
      "Epoch 61/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1821\n",
      "Epoch 62/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1839\n",
      "Epoch 63/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1857\n",
      "Epoch 64/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1756\n",
      "Epoch 65/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1784\n",
      "Epoch 66/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1751\n",
      "Epoch 67/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1746\n",
      "Epoch 68/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1728\n",
      "Epoch 69/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1715\n",
      "Epoch 70/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1678\n",
      "Epoch 71/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1647\n",
      "Epoch 72/100\n",
      "31211/31211 [==============================] - 41s 1ms/step - loss: 0.1659\n",
      "Epoch 73/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1665\n",
      "Epoch 74/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1693\n",
      "Epoch 75/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1625\n",
      "Epoch 76/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.1677\n",
      "Epoch 77/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1605\n",
      "Epoch 78/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1667\n",
      "Epoch 79/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1673\n",
      "Epoch 80/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1694\n",
      "Epoch 81/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.1671\n",
      "Epoch 82/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1591\n",
      "Epoch 83/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1657\n",
      "Epoch 84/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1609\n",
      "Epoch 85/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1668\n",
      "Epoch 86/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1660\n",
      "Epoch 87/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1626\n",
      "Epoch 88/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1620\n",
      "Epoch 89/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1562\n",
      "Epoch 90/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1600\n",
      "Epoch 91/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1550\n",
      "Epoch 92/100\n",
      "31211/31211 [==============================] - 40s 1ms/step - loss: 0.1575\n",
      "Epoch 93/100\n",
      "31211/31211 [==============================] - 41s 1ms/step - loss: 0.1627\n",
      "Epoch 94/100\n",
      "31211/31211 [==============================] - 38s 1ms/step - loss: 0.1595\n",
      "Epoch 95/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1562\n",
      "Epoch 96/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1559\n",
      "Epoch 97/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1596\n",
      "Epoch 98/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1523\n",
      "Epoch 99/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1581\n",
      "Epoch 100/100\n",
      "31211/31211 [==============================] - 39s 1ms/step - loss: 0.1558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14a3d9630>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    # We will use this during training to see if model is improving\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for temperature in [1.5, 0.75, 0.25]:\n",
    "        print('----- temperature:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = \"shall i compare thee with a summers day?\"\n",
    "        generated += sentence\n",
    "        print('----- Seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(100):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=100,\n",
    "          use_multiprocessing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Generating with seed: \"shall i compare thee with a summers day?\"\n",
      "shall i compare thee with a summers day?\n",
      " and seemize the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessiotamborini/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requard them time to p,\n",
      "lat, me fear thou of your pore my swee,\n",
      "t nothing sweet sking, but when my glass,\n",
      " some shows my deceeds but their deep ,\n",
      "some say more tortment, some int, to shu,\n",
      "et for a the me. i see of were i am an,\n",
      "d seem, blant do the ward on thy hand, an,\n",
      "d seeming beaut as augh raintane, as t,\n",
      "hou thust dread the dear resping eat, me,\n",
      " and lay, and all my singlang, but no se,\n",
      "l sheel she see jul summer's face sojelv,\n",
      "ess by name the consents, and time of my ,\n",
      "live in thy heart thy songue as for thee,,\n",
      " thought is best beauty heart all you liv,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_seed = \"shall i compare thee with a summers day?\"\n",
    "\n",
    "print()\n",
    "diversity = 0.25\n",
    "generated = ''\n",
    "sentence = \"shall i compare thee with a summers day?\"\n",
    "generated += sentence\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated + '\\n')\n",
    "\n",
    "\n",
    "structure = [4,4,4,2]\n",
    "\n",
    "for s in structure:\n",
    "    for l in range(s):\n",
    "        num = random.randrange(38, 42)\n",
    "        for i in range(num):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        sys.stdout.write(',\\n')\n",
    "        sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
