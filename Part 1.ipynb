{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hannahfan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/hannahfan/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM_helper import (\n",
    "    text_to_wordcloud,\n",
    "    states_to_wordclouds,\n",
    "    sample_sentence,\n",
    "    visualize_sparsities,\n",
    "    animate_emission,\n",
    "    obs_map_reverser\n",
    ")\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_observations(lines):\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "filename = 'data/shakespeare.txt'\n",
    "filename_2 = 'data/spenser.txt'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    with open(filename_2, 'r') as f2:\n",
    "        content = f.readlines() + f2.readlines()\n",
    "        lines = []\n",
    "        for line in content:\n",
    "            line = line.strip(',')\n",
    "            if len(line) > 23:\n",
    "                words = (line.split('\\n')[0].split(' '))\n",
    "                while '' in words : words.remove('')\n",
    "                newWords = []\n",
    "                for w in words:\n",
    "                    w = w.strip(' ,.:;?!\\'()').lower()\n",
    "                    newWords.append(w)\n",
    "                lines.append(newWords)\n",
    "\n",
    "obs, obs_map = parse_observations(lines)\n",
    "\n",
    "# Syllable data\n",
    "with open('data/Syllable_dictionary.txt', 'r') as syl_f:\n",
    "    syl_content = syl_f.readlines()\n",
    "word_to_syl = {}\n",
    "for l in syl_content:\n",
    "    spl = l.split()\n",
    "    if len(spl) == 3 and (spl[2] == 'E3' or spl[2] == 'E4'):\n",
    "        word_to_syl[spl[0]] = spl[1]\n",
    "    else:\n",
    "        word_to_syl[spl[0]] = spl[len(spl)-1]\n",
    "syllables = []\n",
    "for l in lines:\n",
    "    syl_line = []\n",
    "    for w in l:\n",
    "        if w in word_to_syl.keys():\n",
    "            syl_line.append(int(word_to_syl.get(w)))\n",
    "        else:\n",
    "            new_w = \"\\'\" + w\n",
    "            if new_w in word_to_syl.keys():\n",
    "                syl_line.append(int(word_to_syl.get(new_w)))\n",
    "            else:\n",
    "                syl_line.append(0)\n",
    "    syllables.append(syl_line)\n",
    "\n",
    "# Part of speech data\n",
    "parts_of_speech = []\n",
    "pos_map = {}\n",
    "pos_counter = 0\n",
    "for l in lines:\n",
    "    pos_line = []\n",
    "    for w in l:\n",
    "        pos = nltk.pos_tag([w])[0][1]\n",
    "        if pos not in pos_map:\n",
    "            pos_map[pos] = pos_counter\n",
    "            pos_counter += 1\n",
    "        \n",
    "        pos_line.append(pos_map[pos])\n",
    "    \n",
    "    parts_of_speech.append(pos_line)\n",
    "\n",
    "# Initialize a 2D dictionary for syllables and parts of speech\n",
    "syl_pos_dict = dict()\n",
    "for d in range(6): \n",
    "    syl_pos_dict[d] = dict()\n",
    "    \n",
    "for l in syl_content:\n",
    "    spl = l.split()\n",
    "    v = spl[0]\n",
    "    pos = pos_map[nltk.pos_tag([v])[0][1]]\n",
    "    \n",
    "    # emphasis handling\n",
    "    if len(spl) == 3 and (spl[2] == 'E3' or spl[2] == 'E4'):\n",
    "        k = int(spl[1])\n",
    "    else:\n",
    "        k = int(spl[len(spl)-1])\n",
    "    \n",
    "    if pos in syl_pos_dict[k].keys():\n",
    "        syl_pos_dict[k][pos].append(obs_map[v.strip('\\'')])\n",
    "    else:\n",
    "        syl_pos_dict[k][pos] = [obs_map[v.strip('\\'')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IN': 0, 'NN': 1, 'NNS': 2, 'PRP': 3, 'RB': 4, 'VBD': 5, 'MD': 6, 'CC': 7, 'DT': 8, 'PRP$': 9, 'VBN': 10, 'TO': 11, 'JJ': 12, 'VBG': 13, 'WRB': 14, 'VB': 15, 'RBR': 16, 'VBZ': 17, 'WP$': 18, 'WP': 19, 'VBP': 20, 'WDT': 21, 'CD': 22, 'JJS': 23, 'JJR': 24}\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "print(pos_map)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, obs_map = parse_observations(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n"
     ]
    }
   ],
   "source": [
    "hmm_words = unsupervised_HMM(obs, 4, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n"
     ]
    }
   ],
   "source": [
    "hmm_pos = unsupervised_HMM(parts_of_speech, 4, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n"
     ]
    }
   ],
   "source": [
    "hmm_syls = unsupervised_HMM(syllables, 4, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine the results from an HMM trained on the words and an HMM trained on the syllables of the sonnets to\n",
    "create a sentence of desired length\n",
    "\n",
    "Input:\n",
    "    - word_hmm: HMM trained on input of the words of the sonnets\n",
    "    - syllable_hmm: HMM trained on input of the syllables of words of the sonnets\n",
    "    - syllables_to_words: dictionary with keys being number of syllables and values being lists of words\n",
    "        with the corresponding number of syllables\n",
    "    - obs_map: dictionary mapping words to their respective integer encoding\n",
    "    - n_syllables: number of syllables for the sentence to have\n",
    "    \n",
    "Output: A string sentence of length n_syllables syllables\n",
    "\"\"\"\n",
    "def create_sentence(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables):\n",
    "    emission = []\n",
    "    \n",
    "    # Generate sequences to construct our sentence from\n",
    "    syl_seq = syllable_hmm.generate_emission(n_syllables)[0]\n",
    "    pos_seq = pos_hmm.generate_emission(n_syllables)[0]\n",
    "    states = word_hmm.generate_emission(n_syllables)[1]\n",
    "    \n",
    "    try:\n",
    "        # Initial word\n",
    "        indices = syl_pos_dict[syl_seq[0]][pos_seq[0]]\n",
    "        probs_sum = sum([word_hmm.O[0][i] for i in indices])\n",
    "        probs = [word_hmm.O[0][i] / probs_sum for i in indices]\n",
    "        word = np.random.choice(indices, p=probs)\n",
    "        emission.append(word)\n",
    "\n",
    "        syl_counter = syl_seq[0]\n",
    "        idx = 1\n",
    "        # Get new words until we reach the given number of syllables\n",
    "        while syl_counter < n_syllables:\n",
    "            syls = syl_seq[idx]\n",
    "            if syls + syl_counter > n_syllables: syls = n_syllables - syl_counter\n",
    "            syl_counter += syls\n",
    "                \n",
    "            indices = syl_pos_dict[syls][pos_seq[idx]]\n",
    "            probs_sum = sum([word_hmm.O[states[idx]][i] for i in indices])\n",
    "            probs = [word_hmm.O[states[idx]][i] / probs_sum for i in indices]\n",
    "            word = np.random.choice(indices, p=probs)\n",
    "            emission.append(word)\n",
    "            \n",
    "            idx += 1\n",
    "        \n",
    "        # Convert the encoded words back to English words\n",
    "        obs_map_rev = obs_map_reverser(obs_map)\n",
    "        sentence_array = [obs_map_rev[i] for i in emission]\n",
    "\n",
    "        sentence = ' '.join(sentence_array).capitalize()\n",
    "        return sentence\n",
    "    \n",
    "    # If we generate sequences which give a combination of number of syllables and part of speech\n",
    "    # that is not found in the dictionary, try again\n",
    "    except KeyError as e:\n",
    "        create_sentence(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine the results from an HMM trained on the words and an HMM trained on the syllables of the sonnets to\n",
    "create a sentence of desired length starting with a given word as the last word in the sentence\n",
    "\n",
    "Input:\n",
    "    - word_hmm: HMM trained on input of the words of the sonnets\n",
    "    - syllable_hmm: HMM trained on input of the syllables of words of the sonnets\n",
    "    - syllables_to_words: dictionary with keys being number of syllables and values being lists of words\n",
    "        with the corresponding number of syllables\n",
    "    - obs_map: dictionary mapping words to their respective integer encoding\n",
    "    - n_syllables: number of syllables for the sentence to have\n",
    "    - init_word: encoding of the last word in the sentence\n",
    "    \n",
    "Output: A string sentence of length n_syllables syllables\n",
    "\"\"\"\n",
    "def create_sentence_reverse(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables, init_word):\n",
    "    emission = [init_word]\n",
    "    \n",
    "    # Generate sequences to build the sentence from\n",
    "    syl_seq = syllable_hmm.generate_emission(n_syllables)[0]\n",
    "    pos_seq = pos_hmm.generate_emission(n_syllables)[0]\n",
    "    \n",
    "    # Find the initial state based on the state with the highest probability of getting the given word\n",
    "    init_probs_sum = sum([word_hmm.O[i][init_word] for i in range(word_hmm.L)])\n",
    "    init_probs = [word_hmm.O[i][init_word] / init_probs_sum for i in range(word_hmm.L)]\n",
    "    state = np.random.choice(range(word_hmm.L), p=init_probs)\n",
    "    \n",
    "    # Initialize syllable counter\n",
    "    obs_map_rev = obs_map_reverser(obs_map)\n",
    "    syl_counter = int(word_to_syl[obs_map_rev[init_word]])\n",
    "    \n",
    "    try:\n",
    "        # Start with the second-to-last values in the sequence\n",
    "        indices = syl_pos_dict[syl_seq[n_syllables - 2]][pos_seq[n_syllables - 2]]\n",
    "        probs_sum = sum([word_hmm.O[state][i] for i in indices])\n",
    "        probs = [word_hmm.O[state][i] / probs_sum for i in indices]\n",
    "        word = np.random.choice(indices, p=probs)\n",
    "        emission.append(word)\n",
    "        \n",
    "        syl_counter += syl_seq[n_syllables - 2]\n",
    "        idx = n_syllables - 3\n",
    "        while syl_counter < n_syllables:\n",
    "            syls = syl_seq[idx]\n",
    "            if syls + syl_counter > n_syllables: syls = n_syllables - syl_counter\n",
    "            syl_counter += syls\n",
    "                \n",
    "            indices = syl_pos_dict[syls][pos_seq[idx]]\n",
    "            \n",
    "            # Backtrack to choose a previous state based on probabilities in the transition matrix\n",
    "            state_probs_sum = sum([word_hmm.A[i][state] for i in range(word_hmm.L)])\n",
    "            state_probs = [word_hmm.A[i][state] / state_probs_sum for i in range(word_hmm.L)]\n",
    "            state = np.random.choice(range(word_hmm.L), p=state_probs)\n",
    "            \n",
    "            probs_sum = sum([word_hmm.O[state][i] for i in indices])\n",
    "            probs = [word_hmm.O[state][i] / probs_sum for i in indices]\n",
    "            word = np.random.choice(indices, p=probs)\n",
    "            emission.append(word)\n",
    "            \n",
    "            idx -= 1\n",
    "        \n",
    "        # Reverse the emission sequence and convert back to English words\n",
    "        emission = emission[::-1]\n",
    "        \n",
    "        sentence_array = [obs_map_rev[i] for i in emission]\n",
    "        sentence = ' '.join(sentence_array).capitalize()\n",
    "        return sentence\n",
    "    \n",
    "    # If we generate sequences which give a combination of number of syllables and part of speech\n",
    "    # that is not found in the dictionary, try again\n",
    "    except KeyError as e:\n",
    "        return create_sentence_reverse(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables, init_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables):\n",
    "    \"\"\"\n",
    "    In the generate emission poem function we want to generate emissions in the form of a the poem:\n",
    "    3 quatrain, 1 couplet. \n",
    "    \"\"\"\n",
    "    poem_emissions = []\n",
    "    poem_states = []\n",
    "\n",
    "    sonnet_structure = [4,4,4,2] #number of lines per structure in sonnet\n",
    "    for num in sonnet_structure:\n",
    "        structure_emission = []\n",
    "        \n",
    "        for n in range(num): \n",
    "            line_emission = create_sentence(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map,n_syllables) \n",
    "            while type(line_emission) == type(None): \n",
    "                line_emission = create_sentence(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map,n_syllables) \n",
    "            if n != num-1:\n",
    "                structure_emission.append(line_emission + ',\\n')\n",
    "            else: \n",
    "                structure_emission.append(line_emission + '.\\n')\n",
    "        poem_emissions.append(''.join(structure_emission))\n",
    "\n",
    "\n",
    "    return ''.join(poem_emissions)\n",
    "\n",
    "words_that_rhyme = [\n",
    "    [\"increase\", \"decease\"], [\"brow\", \"now\"], [\"mother\", \"another\"], [\"womb\", \"tomb\"], [\"mine\", \"thine\"],\n",
    "    [\"eyes\", \"lies\"], [\"excuse\", \"use\"], [\"fair\", \"heir\"], [\"art\", \"depart\"], [\"day\", \"way\"], [\"perish\", \"cherish\"],\n",
    "    [\"cold\", \"old\"], [\"decay\", \"away\"], [\"thereby\", \"die\"], [\"time\", \"prime\"], [\"night\", \"white\"], [\"go\", \"grow\"],\n",
    "    [\"be\", \"thee\"], [\"live\", \"give\"], [\"time\", \"rhyme\"], [\"fair\", \"repair\"], [\"men\", \"pen\"], [\"still\", \"skill\"],\n",
    "    [\"cruel\", \"fuel\"], [\"look\", \"book\"], [\"due\", \"true\"], [\"power\", \"hour\"], [\"strong\", \"wrong\"], [\"fly\", \"eye\"]\n",
    "]\n",
    "def generate_rhyming_poem(word_hmm, syllable_hmm, pos_hmm, syl_pos_dict, obs_map, n_syllables):\n",
    "    \"\"\"\n",
    "    In the generate emission poem function we want to generate emissions in the form of a the poem:\n",
    "    3 quatrain, 1 couplet. \n",
    "    \"\"\"\n",
    "    poem_emissions = []\n",
    "    poem_states = []\n",
    "\n",
    "    sonnet_structure = [4,4,4,2] #number of lines per structure in sonnet\n",
    "    for num in sonnet_structure:\n",
    "        structure_emission = []\n",
    "        rhymes = [random.choice(words_that_rhyme) for _ in range(num)]\n",
    "        r = 0\n",
    "        \n",
    "        for l in range(int(num / 2)):\n",
    "            if num > 2:\n",
    "                # Alternating rhymes\n",
    "                rhyme_word_a = obs_map[rhymes[int(r / 2)][r % 2]]\n",
    "                rhyme_word_b = obs_map[rhymes[int(r / 2 + 1)][r % 2]]\n",
    "                r += 1\n",
    "\n",
    "                # Consecutive rhymes\n",
    "                \"\"\"rhyme_word_a = obs_map[rhymes[int(r / 2)][r % 2]]\n",
    "                rhyme_word_b = obs_map[rhymes[int(r / 2)][(r + 1) % 2]]\n",
    "                r += 2\"\"\"\n",
    "            else:\n",
    "                rhyme_word_a = obs_map[rhymes[int(r / 2)][r % 2]]\n",
    "                rhyme_word_b = obs_map[rhymes[int(r / 2)][(r + 1) % 2]]\n",
    "                r += 2\n",
    "            \n",
    "            line_emission = create_sentence_reverse(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10, rhyme_word_a)\n",
    "            \n",
    "            if l * 2 != num-1:\n",
    "                structure_emission.append(line_emission + ',\\n')\n",
    "            else: \n",
    "                structure_emission.append(line_emission + '.\\n')\n",
    "            \n",
    "            line_emission = create_sentence_reverse(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10, rhyme_word_b)\n",
    "            \n",
    "            if l * 2 + 1 != num-1:\n",
    "                structure_emission.append(line_emission + ',\\n')\n",
    "            else: \n",
    "                structure_emission.append(line_emission + '.\\n')\n",
    "                \n",
    "        poem_emissions.append(''.join(structure_emission))\n",
    "\n",
    "\n",
    "    return ''.join(poem_emissions)\n",
    "\n",
    "def sample_sonnet(hmm, obs_map, n_words):\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    \n",
    "    emission, states = generate_emission_poem(n_words)\n",
    "\n",
    "    poem = []\n",
    "    for e in emission:\n",
    "        for i, s in enumerate(e):\n",
    "            sentence = []\n",
    "            for l in s:\n",
    "                sentence.append(obs_map_r[l])\n",
    "            if i != len(e)-1 : \n",
    "                poem.append(' '.join(sentence).capitalize() + ',\\n')\n",
    "            else: \n",
    "                poem.append(' '.join(sentence).capitalize() + '.\\n')\n",
    "            \n",
    "    return ''.join(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good thus ye thee doth beshrew dear love womb,\n",
      "World of love's interim say my cannot womb,\n",
      "Was vow reigned should her bareness where as tomb,\n",
      "Qualify bearer wound of eyes or tomb.\n",
      "Sacred stopped intelligence or life live,\n",
      "Thee that all once a the thought thrusts excuse,\n",
      "Huge they night pitch every doth thee thou give,\n",
      "Justify let art raised thing in art use.\n",
      "Budding in advantage more or cruel,\n",
      "Himself darkness none i do thou will live,\n",
      "Wilt the burn difference a weeds with fuel,\n",
      "Foes sweets by quite which thee is love griefs give.\n",
      "Sing proof i if write are deny cruel,\n",
      "I true still too against be light thou fuel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_rhyming_poem(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More time thine that her again to unto,\n",
      "Above penance his thee made be spirit,\n",
      "Such both reason himself may thou other,\n",
      "Hast in fly worms within let mother's shame.\n",
      "Any sing i woos end i be heart on,\n",
      "Crow so that endured proved upon thou men,\n",
      "Neither much frailties the me i th may the,\n",
      "Her strong our guides proud level doth too.\n",
      "Th i a love i themselves to such long joy,\n",
      "Death loves awake old and subject to part,\n",
      "That corrupt bends every upon themselves,\n",
      "Another allege suffered and as her.\n",
      "Then more my day to doth than you settled,\n",
      "Since from mind loves as wind of another.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course every our shine but flowers old\n",
      "Tear i age be in it worse hell fair cold\n"
     ]
    }
   ],
   "source": [
    "print(create_sentence_reverse(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10, obs_map[\"old\"]))\n",
    "print(create_sentence_reverse(hmm_words, hmm_syls, hmm_pos, syl_pos_dict, obs_map, 10, obs_map[\"cold\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "None\n",
      "1\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n",
      "2\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "None\n",
      "3\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "4\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "5\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "None\n",
      "6\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "None\n",
      "7\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n",
      "8\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n",
      "9\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "None\n",
      "10\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "None\n",
      "11\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "None\n",
      "12\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n",
      "13\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "None\n",
      "14\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "None\n",
      "15\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "None\n",
      "16\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "None\n",
      "17\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "18\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "None\n",
      "19\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "None\n",
      "20\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "None\n",
      "21\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "None\n",
      "22\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "None\n",
      "23\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "None\n",
      "24\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for pos in pos_map.keys():\n",
    "    print(pos_map[pos])\n",
    "    print(nltk.help.upenn_tagset(pos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
